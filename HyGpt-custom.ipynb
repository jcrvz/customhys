{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choices \n",
    "\n",
    "import operators as Operators\n",
    "from metaheuristic import Metaheuristic\n",
    "import benchmark_func as bf\n",
    "from hyperheuristic import Hyperheuristic, _save_step\n",
    "from neural_network import ModelPredictorTransformer, ModelPredictorTransformerOriginal, DatasetSequences\n",
    "from encode_operators import compress_operator, decompress_operator\n",
    "\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_metric as load_metric_hf \n",
    "from datasets import Dataset as Dataset_hf\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_seq = 100\n",
    "\n",
    "seqs, costs = [], []\n",
    "for counting in range(1, 11):\n",
    "  with open(f'vocabulary/seq_read_{counting}.txt', 'r', encoding='utf-8') as file:\n",
    "    seqs = seqs + file.read().split('\\n')  \n",
    "  with open(f'vocabulary/score_{counting}.txt', 'r', encoding='utf-8') as file:\n",
    "    costs = costs + file.read().split('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read operators and find their alias\n",
    "collections = ['default.txt', 'basicmetaheuristics.txt']\n",
    "\n",
    "encoded_heuristic_space = dict()\n",
    "operators_string = dict()\n",
    "for collection_file in collections:\n",
    "    with open('./collections/' + collection_file, 'r') as operators_file:\n",
    "        operators_string[collection_file] = [line.rstrip('\\n') for line in operators_file]\n",
    "        encoded_heuristic_space[collection_file] = [eval(line) for line in operators_string[collection_file]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequence(seq):\n",
    "  operators = []\n",
    "  prev_idx = 0\n",
    "  counting = 0\n",
    "  for i, c in enumerate(seq):\n",
    "    if c == '(':\n",
    "      counting += 1\n",
    "    if c == ')':\n",
    "      counting -= 1\n",
    "      if counting == 0:\n",
    "        operators.append(seq[prev_idx:i+1])\n",
    "        prev_idx = i + 3\n",
    "  return operators\n",
    "\n",
    "def get_ids_operators(operators):\n",
    "  ids = []\n",
    "  for operator in operators:\n",
    "    ids_bool = np.array(operators_string['default.txt']) == operator\n",
    "    ids.append(np.where(ids_bool)[0][0])\n",
    "  return ids\n",
    "\n",
    "def generate_seqs():\n",
    "  seqs_operators = []\n",
    "  seqs_ids = []\n",
    "  for seq in seqs:\n",
    "    operators = parse_sequence(seq)\n",
    "    seq_ids = get_ids_operators(operators)\n",
    "    seqs_operators.append(operators)\n",
    "    seqs_ids.append(seq_ids)\n",
    "  fitnesses = [eval(cost) for cost in costs]\n",
    "  return seqs_operators, seqs_ids, fitnesses\n",
    "\n",
    "      \n",
    "seqs_operators, seqs_ids, fitnesses = generate_seqs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, seqs_ids, fitnesses = generate_seqs()\n",
    "ds = DatasetSequences(seqs_ids, fitnesses, 205, fitness_to_weight='rank')\n",
    "seqs_operators, _, fitnesses = generate_seqs()\n",
    "seqs_compressed_op = [[compress_operator(eval(operator)) for operator in seq] for seq in seqs_operators]\n",
    "ds2 = DatasetSequences(seqs_compressed_op, fitnesses, fitness_to_weight='rank')\n",
    "Xid, yid, fit = ds.obtain_dataset()\n",
    "Xop, yop, _ = ds2.obtain_dataset()\n",
    "\n",
    "A = list(zip(fit, Xop, yid))\n",
    "A.sort(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_seqs = []\n",
    "readable_next = []\n",
    "readable_fitness = fit\n",
    "\n",
    "percentage = 0.3\n",
    "B = A[:int(len(A) * percentage)]\n",
    "for _, xop, y_op in B:\n",
    "  readable_seqs.append(' '.join([str(x) for x in xop]))\n",
    "  readable_next.append(y_op)\n",
    "#  readable_fitness.append(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer():\n",
    "    old_tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "        \"gpt2\",\n",
    "    )   \n",
    "    print(old_tokenizer.pad_token)\n",
    "    old_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer = old_tokenizer.train_new_from_iterator(readable_seqs, vocab_size=30522)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.save_pretrained('vocabulary/HyGpt-tokenizer-compress')\n",
    "    tokenizer.push_to_hub('HyGpt-tokenizer-compress')\n",
    "    return tokenizer\n",
    "#tokenizer = train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'file_label': 'HyGpt-compress-tests-class',\n",
    "    'num_steps': 100,\n",
    "    'num_operators': 205,\n",
    "    \"load_model\": True,\n",
    "    \"save_model\": False,\n",
    "    \"encoder\": \"identity\",\n",
    "    \"model_architecture\": \"transformer\",\n",
    "    \"pretrained_tokenizer\" : \"josetapia/HyGpt-tokenizer-compress\",\n",
    "    \"pretrained_model\": \"gpt2\",\n",
    "    \"epochs\": 2,\n",
    "    \"fitness_to_weight\": \"rank\",\n",
    "    \"sample_params\": {\n",
    "      \"retrieve_sequences\": False,\n",
    "      \"limit_seqs\": 400,\n",
    "      \"filter\": \"first_quartile\",\n",
    "      \"store_sequences\": False\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelPredictorTransformer(params)\n",
    "model._tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function ModelPredictorTransformer.fit.<locals>.<lambda> at 0x2dcfa4d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 95/95 [01:41<00:00,  1.07s/ba]\n",
      "  0%|          | 0/8865 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model.fit(readable_seqs, readable_next, 3, readable_fitness, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._trainer.save_model('HyGpt/hygpt-class-compress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._tokenizer.pad_token_id, model._tokenizer.pad_token, model._tokenizer.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model._tokenizer(readable_seqs[:200], max_length=1024, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(a) for a in A[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13a1a4c7f867d5ec33b811066f10a0e467cde7b9dca366a8ff958366b3d34ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
