{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "from mpmath import *\n",
    "from decimal import Decimal\n",
    "from timeit import default_timer as timer\n",
    "import json\n",
    "\n",
    "import tools as tl\n",
    "import benchmark_func as bf\n",
    "from hyperheuristic import Hyperheuristic\n",
    "from metaheuristic import Metaheuristic\n",
    "from experiment import read_config_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context=\"paper\", font_scale=1.8, palette=\"husl\", style=\"ticks\",\n",
    "        rc={'font.family': 'serif', 'font.size': 18,\n",
    "            \"xtick.major.top\": False, \"ytick.major.right\": False})\n",
    "plt.rc('font', size=18) \n",
    "\n",
    "# Saving images flag\n",
    "is_saving = True\n",
    "show_plots = True\n",
    "saving_format = 'png'\n",
    "\n",
    "figures_folder = 'data_files/exp_figures/'\n",
    "experiment_folder = 'data_files/exp_output'\n",
    "ml_model_folder = \"./data_files/ml_models_times/\"\n",
    "exp_label = 'default_nn_best_double_lstm'\n",
    "bm_exp_label = 'basic_metaheuristic'\n",
    "\n",
    "if is_saving:\n",
    "    # Read (or create if so) a folder for storing images\n",
    "    if not os.path.isdir(figures_folder):\n",
    "        os.mkdir(figures_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, [2, 10, 30, 50])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(ml_model_folder)\n",
    "files = [file for file in files if 'DS_S' not in file]\n",
    "\n",
    "# Number of dimensions\n",
    "dimensions = [2, 10, 30, 50]\n",
    "num_dims = len(dimensions)\n",
    "\n",
    "problems = list(set(file.split('-')[0] for file in files))\n",
    "len(problems), dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tl.read_json('./data_files/exp_output/'+exp_label+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dataset(dataset):\n",
    "    # Sort the results of the 428 problems in lexicographically order\n",
    "    list_pair_results = list(zip(dataset['problem'],\n",
    "                                 dataset['dimensions'],\n",
    "                                 dataset['results']))\n",
    "    list_pair_results.sort()\n",
    "    dataset['problem'], dataset['dimensions'], dataset['results'] = [], [], []\n",
    "    for problem, dimension, result in list_pair_results:\n",
    "        dataset['problem'].append(problem)\n",
    "        dataset['dimensions'].append(dimension)\n",
    "        dataset['results'].append(result)\n",
    "        \n",
    "def filter_by_dimensions(dataset):\n",
    "    # Filter the dataset allowing only the dimensions from 'dimensions' list.\n",
    "    # The usercase is for the basic metaheuristics that has results for extra \n",
    "    # dimensions out of scope of this paper.\n",
    "    allowed_dim_inds = [index for d in dimensions for index in tl.listfind(dataset['dimensions'], d)]\n",
    "    dict_filtered = {key: [val[x] for x in allowed_dim_inds] for key, val in dataset.items()}\n",
    "    sort_dataset(dict_filtered)\n",
    "    return dict_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read operators and find their alias\n",
    "collections = ['default.txt', 'basicmetaheuristics.txt']\n",
    "\n",
    "encoded_heuristic_space = dict()\n",
    "for collection_file in collections:\n",
    "    with open('./collections/' + collection_file, 'r') as operators_file:\n",
    "        encoded_heuristic_space[collection_file] = [eval(line.rstrip('\\n')) for line in operators_file]\n",
    "\n",
    "# Search operator aliases\n",
    "perturbator_alias = {\n",
    "    'random_search': 'RS',\n",
    "    'central_force_dynamic': 'CF',\n",
    "    'differential_mutation': 'DM',\n",
    "    'firefly_dynamic': 'FD',\n",
    "    'genetic_crossover': 'GC',\n",
    "    'genetic_mutation': 'GM',\n",
    "    'gravitational_search': 'GS',\n",
    "    'random_flight': 'RF',\n",
    "    'local_random_walk': 'RW',\n",
    "    'random_sample': 'RX',\n",
    "    'spiral_dynamic': 'SD',\n",
    "    'swarm_dynamic': 'PS'}\n",
    "\n",
    "selector_alias = {'greedy': 'g', 'all': 'd', 'metropolis': 'm', 'probabilistic': 'p'}\n",
    "\n",
    "operator_families = {y: i for i, y in enumerate(sorted([x for x in perturbator_alias.values()]))}\n",
    "\n",
    "# Pre-build the alias list\n",
    "heuristic_space = dict()\n",
    "for collection_file in collections:\n",
    "    if all(isinstance(x, tuple) for x in encoded_heuristic_space[collection_file]):\n",
    "        heuristic_space[collection_file] = [perturbator_alias[x[0]] + selector_alias[x[2]]\n",
    "                                            for x in encoded_heuristic_space[collection_file]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read basic metaheuristics\n",
    "basic_mhs_collection = encoded_heuristic_space['basicmetaheuristics.txt']\n",
    "\n",
    "# Read basic metaheuristics cardinality\n",
    "basic_mhs_cadinality = [1 if isinstance(x, tuple) else len(x) for x in basic_mhs_collection]\n",
    "\n",
    "# Load data from basic metaheuristics\n",
    "basic_mhs_data = filter_by_dimensions(tl.read_json(f'{experiment_folder}/basic-metaheuristics-data_v2.json'))\n",
    "\n",
    "# Mathematical attributes\n",
    "chosen_categories = ['Differentiable', 'Unimodal']\n",
    "case_label = 'DU'\n",
    "\n",
    "# Call the problem categories\n",
    "problem_features = bf.list_functions(fts=chosen_categories)\n",
    "categories = sorted(set([problem_features[x]['Code'] for x in basic_mhs_data['problem']]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the results per experiment\n",
    "data_frame = filter_by_dimensions(data)\n",
    "\n",
    "# Summary\n",
    "id = 'dlstm'\n",
    "data_info = {\n",
    "    'Dim': [int(x) for x in data_frame['dimensions']],\n",
    "    'Pop': 30,\n",
    "    'Problem': data_frame['problem'],\n",
    "    'Cat': [problem_features[x]['Code'] for x in data_frame['problem']],\n",
    "    'mhs': [x['encoded_solution'] for x in data_frame['results']],\n",
    "    'hist': [x['hist_fitness'] for x in data_frame['results']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_id(problem_name, dimension):\n",
    "  for id in range(len(data_info['Problem'])):\n",
    "    correct_problem = data_info['Problem'][id] == problem_name\n",
    "    correct_dimension = data_info['Dim'][id] == dimension\n",
    "    if correct_problem and correct_dimension:\n",
    "      return id\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "exp_config, hh_config, prob_config = read_config_file(exp_label)\n",
    "# Message to print and to store in folders\n",
    "problem_name = 'Sphere'\n",
    "dimension = 2\n",
    "label = '{}-{}D-{}'.format(problem_name, dimension, exp_label)\n",
    "\n",
    "# Get and format the problem\n",
    "# problem = eval('bf.{}({})'.format(problem, dimension))\n",
    "problem = bf.choose_problem(problem_name, dimension)\n",
    "\n",
    "problem.set_offset_domain(2)\n",
    "problem.set_offset_function(3)\n",
    "problem.set_scale_domain(1.5)\n",
    "problem.set_scale_function(4)\n",
    "problem.set_noise_type('normal')\n",
    "problem.set_noise_level(2)\n",
    "\n",
    "problem_to_solve = problem.get_formatted_problem(True, ['Differentiable', 'Separable', 'Unimodal'])\n",
    "from optproblems.cec2005 import F1\n",
    "fun = F1(2, None)\n",
    "problem_to_solve['function'] = fun\n",
    "\n",
    "# Call the hyper-heuristic object\n",
    "hh = Hyperheuristic(heuristic_space=exp_config['heuristic_collection_file'],\n",
    "                        problem=problem_to_solve, parameters=hh_config,\n",
    "                        file_label=label, weights_array=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_mhs(problem_name, dimension):\n",
    "  id = get_correct_id(problem_name, dimension)\n",
    "  hists = [historial[-1] for historial in data_info['hist'][id]]\n",
    "  optimal_position = np.argmin(hists)\n",
    "  return data_info['mhs'][id][optimal_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "exp_config, hh_config, prob_config = read_config_file(exp_label)\n",
    "\n",
    "\n",
    "# Get and format the problem\n",
    "# problem = eval('bf.{}({})'.format(problem, dimension))\n",
    "problem_name = 'Sphere'\n",
    "dimension = 10\n",
    "problem = bf.choose_problem(problem_name, dimension)\n",
    "problem_to_solve = problem.get_formatted_problem(True, ['Differentiable', 'Separable', 'Unimodal'])\n",
    "\n",
    "label = '{}-{}D-{}'.format(problem_name, dimension, exp_label)\n",
    "hh_config['verbose'] = False\n",
    "hh = Hyperheuristic(heuristic_space=exp_config['heuristic_collection_file'],\n",
    "                    problem=problem_to_solve, parameters=hh_config,\n",
    "                    file_label=label, weights_array=None)\n",
    "\n",
    "start_time = timer()\n",
    "hh.parameters['num_replicas'] = 1000\n",
    "results = hh._solve_neural_network()\n",
    "\n",
    "end_time = timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "encoded_heuristic_space['default.txt'][:5]\n",
    "str(encoded_heuristic_space['default.txt'][0])\n",
    "\n",
    "# Convert\n",
    "sequences = results[1]\n",
    "collection = encoded_heuristic_space['default.txt']\n",
    "sequences_readable = [', '.join(\n",
    "  [str(collection[operator]) for operator in seq[1:]]\n",
    "  ) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('vocabulary'):\n",
    "  os.mkdir('vocabulary')\n",
    "\n",
    "counting = 0\n",
    "limit_seq = 100\n",
    "paths = []\n",
    "for i in range(0, len(sequences_readable), limit_seq):\n",
    "  counting += 1\n",
    "  with open(f'vocabulary/seq_read_{counting}_2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(sequences_readable[i:i+limit_seq]))\n",
    "  with open(f'vocabulary/score_{counting}_2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join([str(x) for x in results[0][i:i+limit_seq]]))\n",
    "  paths.append(f'vocabulary/seq_read_{counting}_2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(seq) for seq in sequences_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11421"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(seq) for seq in sequences_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hybertheuristic/vocab.json', 'hybertheuristic/merges.txt']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "# initialize\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=1024, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "model_name = 'hybertheuristic'\n",
    "if not os.path.isdir(model_name):\n",
    "  os.mkdir(model_name)\n",
    "\n",
    "tokenizer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 1024\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = [tokenize(seq) for seq in sequences_readable]\n",
    "len(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 212kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 86.2M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 3, 1024])\n",
      "attention_mask shape: torch.Size([5, 3])\n",
      "labels shape: torch.Size([5, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/josetapia/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/josetapia/customhys_transformer into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"customhys_transformer\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    #fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenized_datasets[i]['input_ids']) for i in range(1000)])#for x in tokenized_datasets[i]['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:708\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 708\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    710\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    711\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    712\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a sequence",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/josetapia/dev/customhys/transformer_tokenizer.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josetapia/dev/customhys/transformer_tokenizer.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/trainer.py:1374\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1373\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1374\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1375\u001b[0m \n\u001b[1;32m   1376\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1378\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/data/data_collator.py:41\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf_call(features)\n\u001b[1;32m     40\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch_call(features)\n\u001b[1;32m     42\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/data/data_collator.py:729\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtorch_call\u001b[39m(\u001b[39mself\u001b[39m, examples: List[Union[List[\u001b[39mint\u001b[39m], Any, Dict[\u001b[39mstr\u001b[39m, Any]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    727\u001b[0m     \u001b[39m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(examples[\u001b[39m0\u001b[39m], (\u001b[39mdict\u001b[39m, BatchEncoding)):\n\u001b[0;32m--> 729\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(examples, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of)\n\u001b[1;32m    730\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m         batch \u001b[39m=\u001b[39m {\n\u001b[1;32m    732\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: _torch_collate_batch(examples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    733\u001b[0m         }\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2862\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2859\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m   2860\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m-> 2862\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:213\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    209\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:724\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    720\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m             )\n\u001b[0;32m--> 724\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    729\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('genetic_crossover', {'pairing': 'random', 'crossover': 'linear_0.5_0.5', 'mating_pool_factor': 0.4}, 'greedy')\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_readable[0][:112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 277, 299, 67, 275, 263, 280, 307, 262, 261, 318, 263, 261, 275, 262, 261, 398, 67, 20, 18, 25, 67, 20, 18, 25, 263, 261, 306, 67, 304, 67, 290, 262, 291, 18, 24, 278, 261, 342, 276, 2]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sequences_readable[0][:112])['input_ids']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', \"('\", 'genetic', '_', 'crossover', \"',\", \"Ġ{'\", 'pairing', \"':\", \"Ġ'\", 'random', \"',\", \"Ġ'\", 'crossover', \"':\", \"Ġ'\", 'linear', '_', '0', '.', '5', '_', '0', '.', '5', \"',\", \"Ġ'\", 'mating', '_', 'pool', '_', 'factor', \"':\", 'Ġ0', '.', '4', '},', \"Ġ'\", 'greedy', \"')\", '</s>']\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_ids_to_tokens(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base (https://huggingface.co/t5-base)\n",
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 540kB/s]\n",
      "Downloading: 100%|██████████| 850M/850M [02:09<00:00, 6.89MB/s] \n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 2.06MB/s]\n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 2.98MB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"'swarm_dynamic': 'pairing': 'random\"}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text2text_generator = pipeline(\"text2text-generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"'swarm_dynamic': 1.0, 'pairing': '\"}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2text_generator(sequences_readable[0][:512], min_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13a1a4c7f867d5ec33b811066f10a0e467cde7b9dca366a8ff958366b3d34ba1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
